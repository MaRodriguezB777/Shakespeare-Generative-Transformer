{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaRodriguezB777/Shakespeare-Generative-Transformer/blob/main/Tiny_Shakespeare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3oZbYfCAHPl",
        "outputId": "765d9e01-a783-42e1-c684-2ae93ddd2186"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Adapted from Andrej Karpathy's Generative Transformer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from urllib.request import urlopen\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "#hyperparams\n",
        "block_size = 64 # Token size / Recall size\n",
        "batch_size = 256 # Number of simultaneous training points\n",
        "epochs = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_heads = 6\n",
        "dropout = 0.2\n",
        "n_layers = 6\n",
        "# ----------\n",
        "\n",
        "with urlopen(\"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "# get characters from text\n",
        "chars = list(set(text))\n",
        "chars = [chr(int(i)) for i in chars]\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# create mappings, decoder, and encoder for characters to indices and vice-versa\n",
        "stoi = { ch:i for i,ch in enumerate(chars)}\n",
        "itos = { i:ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[ch] for ch in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# train and test splits\n",
        "data = torch.tensor(encode([chr(int(i)) for i in text]), dtype=torch.long)\n",
        "n_split = int(0.9*len(data))\n",
        "train_data = data[:n_split]\n",
        "val_data = data[n_split:]\n",
        "\n",
        "# create batches either for testing or validation\n",
        "def get_batch(split):\n",
        "    data = train_data if split == \"train\" else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
        "    x = torch.stack([data[bix: bix + block_size] for bix in ix]).to(device)\n",
        "    y = torch.stack([data[bix + 1 : bix + block_size + 1] for bix in ix]).to(device)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "# Estimate Loss throughout epochs\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    est = {}\n",
        "    m.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for iter in range(eval_iters):\n",
        "            xb, yb = get_batch(split)\n",
        "            \n",
        "            _, loss = m(xb, yb)\n",
        "            losses[iter] = loss.item()\n",
        "\n",
        "        est[split] = losses.mean(dim=0)\n",
        "    m.train()\n",
        "    return est\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" Creates single head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False) # (B, n_embd, head_size)\n",
        "        self.queries = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.values = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # \"If you have parameters in your model, which should be saved and restored in the state_dict, but not trained by the optimizer, you should register them as buffers.\n",
        "        # Buffers won’t be returned in model.parameters(), so that the optimizer won’t have a change to update them.\" - Guy on discuss.pytorch.org\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) \n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x) # (B, T, head_size)\n",
        "        q = self.queries(x) # (B, T, head_size)\n",
        "        v = self.values(x) # (B, T, head_size)\n",
        "\n",
        "        head_size = k.shape[-1]\n",
        "        # Compares the similarities between each m'th query and each n'th key \n",
        "        w = q @ k.transpose(-1, -2) * head_size**-1/2 # Q * K^T / sqrt(head_size) for each batch\n",
        "        w = w.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # makes it so that contribution to softmax is zero since e^-inf = 0\n",
        "        w = F.softmax(w, dim=-1) # (B, T, T)\n",
        "\n",
        "        self.dropout(w)\n",
        "\n",
        "        return w @ v # (B, T, head_size)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  \n",
        "    def __init__(self, num_heads, head_size):\n",
        "      super().__init__()\n",
        "      self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)]) # (B, num_heads, T, head_size)\n",
        "      self.proj = nn.Linear(num_heads * head_size, num_heads * head_size)\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "      out = torch.cat([head(x) for head in self.heads], dim=-1) # (B, T, head_size * num_heads) want to get all the collected knowledge and combine it (makes it so that not all the attention interacts with itself, i.e. divided into subtasks)\n",
        "      out = self.proj(out)\n",
        "      out = self.dropout(out)\n",
        "\n",
        "      return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\" Linear layer + nonlinearity \"\"\"\n",
        "\n",
        "    def __init__(self, dim):\n",
        "      super().__init__()\n",
        "      self.net = nn.Sequential(\n",
        "          nn.Linear(dim, 4  * dim),\n",
        "          nn.GELU(),\n",
        "          nn.Linear(4 * dim, dim),\n",
        "          nn.Dropout(dropout)\n",
        "      )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "  \"\"\" One transformer block from Attention is All You Need \"\"\"\n",
        "\n",
        "  def __init__(self, n_embd, n_heads):\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_heads\n",
        "    self.att = MultiHeadAttention(n_heads, head_size)\n",
        "    self.ffwd = FeedForward(n_embd)\n",
        "    self.ln1 = nn.LayerNorm(n_embd)\n",
        "    self.ln2 = nn.LayerNorm(n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.ln1(x)\n",
        "    x = x + self.att(x) # (B, T, n_embd)\n",
        "    x= self.ln2(x)\n",
        "    x = x + self.ffwd(x) # (N, T, n_embd)\n",
        "\n",
        "    return self.dropout(x)\n",
        "\n",
        "# Defining model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Each token is embedded into a size of number of vocab letters\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.positional_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(\n",
        "            *[Block(n_embd, n_heads) for _ in range(n_layers)]\n",
        "        )\n",
        "        self.ln = nn.LayerNorm(n_embd)\n",
        "        self.ll_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, input, targets=None):\n",
        "        # idx and target both of dimensions (B, T) where B is batch size and T is time / token dimension\n",
        "        B, T = input.shape\n",
        "        tok_embd = self.token_embedding_table(input) # (B, T, n_embd)\n",
        "        pos_embd = self.positional_embedding_table(torch.arange(T, device=device)) # (T, n_embd) each position gets an embedding\n",
        "        x_embd = tok_embd + pos_embd # (B, T, n_embd)\n",
        "\n",
        "        x = self.blocks(x_embd) # (B, T, n_embd)\n",
        "        preds = self.ll_head(x) # (B, T, vocab_size) Gets the prediction for the next character for each token position\n",
        "\n",
        "        if targets == None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = preds.size()\n",
        "            preds = preds.view(B*T, C) # Says to make each entry in 2d space its own entry and get the corresponding embedding table.\n",
        "            targets = targets.view(B*T) # corresponds to the actual target entry (the index in C dimension of preds) that we see. \n",
        "            loss = F.cross_entropy(preds, targets)\n",
        "            \n",
        "        return preds, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:] # Need to do this so that positional embd has right dimension input (T)\n",
        "            preds, _ = self(idx_cond) # (B, T, C)\n",
        "            \n",
        "            # only care about last prediction\n",
        "            preds = preds[:,-1, :] # (B, C)\n",
        "\n",
        "            probs = F.softmax(preds, dim=1) # (B, C)\n",
        "\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1) / One prediction per batch\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # Want to add this prediction to end of each batch\n",
        "        return idx\n",
        "\n",
        "    def sample(self, max_new_tokens):\n",
        "        return self.generate(torch.zeros((1, 1), dtype=torch.long, device=device), max_new_tokens)[0].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJeyaW6YRGHG"
      },
      "outputs": [],
      "source": [
        "filedir = \"/content/drive/MyDrive/Colab Notebooks/Shakespeare Transformer/\"\n",
        "\n",
        "def save_checkpoint(checkpoint, filename=\"shakespeare_transformer.pth.tar\"):\n",
        "  print(\"=> saving checkpoint\")\n",
        "  torch.save(checkpoint, filedir + filename)\n",
        "\n",
        "def load_checkpoint(checkpoint):\n",
        "  m.load_state_dict(checkpoint['state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUIvqnP4GL1K",
        "outputId": "e13240f0-40c9-4ef0-ed9d-d0700bba2c0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> saving checkpoint\n",
            "Step = 0: train_loss = 1.2787261009216309, val_loss = 1.5784196853637695\n",
            "=> saving checkpoint\n",
            "Step = 500: train_loss = 1.2568527460098267, val_loss = 1.5739822387695312\n"
          ]
        }
      ],
      "source": [
        "load_model = True\n",
        "\n",
        "# Model and optimizer setup\n",
        "m = BigramLanguageModel()\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
        "m.to(device)\n",
        "if load_model:\n",
        "  load_checkpoint(torch.load(filedir + \"shakespeare_transformer.pth.tar\"))\n",
        "\n",
        "# Training the model\n",
        "for epoch in range(1000):\n",
        "\n",
        "    xb, yb = get_batch(\"train\")\n",
        "\n",
        "    if epoch % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "\n",
        "        checkpoint = {\"state_dict\" : m.state_dict(), \"optimizer\" : optimizer.state_dict()}\n",
        "        save_checkpoint(checkpoint)\n",
        "        \n",
        "        print(f\"Step = {epoch}: train_loss = {losses['train']}, val_loss = {losses['val']}\")\n",
        "\n",
        "\n",
        "\n",
        "    preds, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9SiTX02GJua",
        "outputId": "6bde9a13-a21e-4e3b-a98c-cad98a4e9d42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.347420573234558\n"
          ]
        }
      ],
      "source": [
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cvYlrg0KDs6",
        "outputId": "a2bfae22-2335-44f6-f1c9-be2f472c4add"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "POMPEY:\n",
            "Stroke: but must use shall give me swear of\n",
            "Caples Marent chenteni. Thou didst in power\n",
            "For bolt-send-toon me. Which vaven me\n",
            "Is faming, proctifion our amplasious harrity,\n",
            "I kind'd Warwick, partding as mill out.\n",
            "\n",
            "BENVOLIO:\n",
            "With bawd, and brief, my father's coursel.\n",
            "\n",
            "ESCALUS:\n",
            "Am retired, and turn with sunded in horses. A mooner,\n",
            "or evils in you; but your lord,\n",
            "Who courcaised Rutland, and such drught strain\n",
            "Tue forthy as burn did holf thing\n",
            "Is most blood, angen naturate to\n",
            "And the prophes of fearful crown spelf to\n",
            "Thence, so mue forthway I will give them and eye,\n",
            "Withouldst mean, folding tears of distressing, or block.\n",
            "Do how is Coriolanus bunrowed sun\n",
            "reading with wi. Free, spench days on\n",
            "And Henry in aa way, misford of God's gold.\n",
            "Forbur with ground my poing them of the duke,\n",
            "Perjudine prossmity, make time their eest,\n",
            "Were shows aim to set-book it.\n",
            "\n",
            "AULYCES.\n",
            "\n",
            "TRUCH:\n",
            "This is frown'd by changed beauty's as restrans!\n",
            "But a wakement seans, and aiy in yourselves.\n",
            "Stay, unthrink, Catesby, Oxford! to shame,\n",
            "Come that, perase what my redome from my knees,\n",
            "But free well had gewds kings: all post\n",
            "Holp to chaft out in bloods.\n",
            "\n",
            "RTHOP OF GAUREN:\n",
            "No, father, and by Juliet, grave for wise?\n",
            "\n",
            "Third Servant:\n",
            "Now, defy, sir; you have stood, of I make tonger to thee,\n",
            "we'll the fire is redetive to ebsent there.'\n",
            "\n",
            "GLOUCESTER:\n",
            "With ever by-horse, which famous be of his lip,\n",
            "Or scaled all two so fondigatest to?\n",
            "'Hath thus in our devilit-bree which dragons\n",
            "Death hod oriclings'd with wild ne'er mortal:\n",
            "Nor delovered, thou beautting now no shrock,\n",
            "Fall, see slet fash'd frings, do now press the world betwixeth\n",
            "as you as dove him reconce? I was justice\n",
            "Unhappished shoulds forgot me, bear it.\n",
            "\n",
            "SICINIUS:\n",
            "You both, mistress of them, they sleep: beveth thes'\n",
            "matter takes as was and\n",
            "Goldivor very heard Eleve now;\n",
            "And he profaroussion to the proof or humourn\n",
            "The summers of his false wings, thanks;\n",
            "Alas, rice one fornowing thea hidhing,\n",
            "She is for my word, brotherrow or gone's dung! But,\n",
            "Cousin to thy gentleman slowly;--\n",
            "Give me a word's balm;\n",
            "And he were look'd mans so himself and both.\n",
            "Our doom-n, flood with andeard of ounside,\n",
            "To cannoin I wot to it well.\n",
            "\n",
            "POMPEY:\n",
            "It shall be first, great feast, well fit\n",
            "In the wateritgoies, these news has devoution him:\n",
            "Any do in men hang mary thee.\n",
            "\n",
            "ESCALUS:\n",
            "Had fastily to knowful hourr'd thy cousin;\n",
            "But little and envental oppent from them,\n",
            "That revenges to the present froster.\n",
            "\n",
            "LEONTES:\n",
            "His ease that judgmmenters know thee,\n",
            "That as I am high will tender-odds:\n",
            "To murutal-lates cousin nor lands, so I have been?\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "We'll endur it.\n",
            "\n",
            "MENENIUS:\n",
            "Why hast you to off miserly now if\n",
            "you grace will better your entreats?\n",
            "\n",
            "Now, brother:\n",
            "Call her to the norther anger rely.\n",
            "\n",
            "MERCUTIO:\n",
            "Rather cried it, be it so for's Montague,\n",
            "To make, the glory in the tormorousn,\n",
            "Than entreath you; sir, are known like him,\n",
            "Your jemein, could I know for him.\n",
            "\n",
            "Provost:\n",
            "Ay, no, sorry than years then and mine eeds\n",
            "Where defenny inhe a botier place on,\n",
            "Be at the lower--in cornance dead where to-dain\n",
            "Towards my well-son fawn in them?\n",
            "\n",
            "Ground:\n",
            "Well, I hear you had buispather, might have bench\n",
            "That wanting woods growing, and beauty is not.\n",
            "Take likeness dead her speak'd etabbour.\n",
            "\n",
            "KING EDWARD IV:\n",
            "Why quit helds hardly darely hence.\n",
            "\n",
            "MONTAGUE:\n",
            "They trenches; but but he have some here with\n",
            "Who standes forthwith the beast of flatter. His mouth dance\n",
            "That is the dukes death are and of the thron.\n",
            "Now, Gentlemen are my wife\n",
            "To cut Mercury to, send your lawful man, as\n",
            "an a, slaughter with power. Good knows,--\n",
            "\n",
            "MENVOLIO:\n",
            "What, shall undail yet, or then? what should be rust\n",
            "To know fear your cunsit is.\n",
            "3 ANNE:\n",
            "I'll cram, more unby his powery took.\n",
            "No, not not in't osababery,\n",
            "To common thriff and to are mark'd.\n",
            "\n",
            "First Citizen:\n",
            "Would did, sir!\n",
            "\n",
            "DUCHPESS OF YORK:\n",
            "To dead noble Romeo, my senior's never tongue.\n",
            "Pardon Kate in shedly kingly and him dangern'd,\n",
            "Welve's through thy doth begg fall, wilt he ditch.\n",
            "And interity quorns on comins,\n",
            "My kind, and so: but they furzy they will\n",
            "With his fine but upon'd Meanful, the stone,\n",
            "Duin with his made at a draggimys,\n",
            "Shathing my childry torming. Say that, my lord.\n",
            "\n",
            "PETRUCHIS:\n",
            "I'll by thee.\n",
            "\n",
            "EXTON:\n",
            "I have been none feodame in this past timat;\n",
            "I was sate or musicians on:\n",
            "I did, leave you in shax with a sight,\n",
            "Witiff, hold, mother'd with From Lances nor honest.\n",
            "\n",
            "BRAKENBURENBURY:\n",
            "He hath abide. My puutword 'O,'\n",
            "And this then to thy more door adood.\n",
            "\n",
            "TYBROLAND:\n",
            "Stand as blerds, ourselves behind meets my light\n",
            "Toodh pience of the mirnice of nature,\n",
            "Pushian with us in her windert:\n",
            "I'll him serpent out to death--\n",
            "What I charge on him? On my looksh!\n",
            "\n",
            "PRINCE EDWARD:\n",
            "My should far my unsier is so out brown;\n",
            "Full having wars in all Rutkind his.\n",
            "\n",
            "BROWHORE:\n",
            "If I will command at: for what I am for?\n",
            "\n",
            "NORFOLK:\n",
            "Shalt he will have ester head attend this trattle!\n",
            "Good Keen England tho key to Please instanly.\n",
            "But, Perdita, by the Arh, monster\n",
            "Jesungemb. What willshing god unto the danger,\n",
            "That was won there no incpleasing might like\n",
            "Wint carnal set the rip might. But, gone.\n",
            "\n",
            "WARWICK:\n",
            "What!\n",
            "Art thou, sorry to York me hast uncreading me\n",
            "Already redels seem and bid uts.\n",
            "\n",
            "KITHAR:\n",
            "I shall have purt hence to thy counsels;\n",
            "But that would it every to sight me, or--\n",
            "\n",
            "Nurse:\n",
            "Marry, whule I will peace?\n",
            "\n",
            "Most mENER:\n",
            "Though my glory, sir.\n",
            "\n",
            "MERTIUS:\n",
            "Now, the if takes mistake yours: hold you God,\n",
            "Flay it were a gozen vouchus rate awn.\n",
            "\n",
            "BRUTUS:\n",
            "\n",
            "Your ROF PAULER:\n",
            "Fiends forth, book for Faven, hokes it come;\n",
            "Thou hast grannibled in his clouds orch speen\n",
            "And sland the cypeersed of our gross, and I\n",
            "Iron coce the glorious voices of footing to earth\n",
            "Than whooer I pleeding, apiciange\n",
            "Tattern most for that and makest my millsth--\n",
            "\n",
            "CAMILLIUS:\n",
            "Forbid one a' go in the day, I was not:\n",
            "Go thour woe's boar! bend ther surfeids.\n",
            "\n",
            "Son BRAKENBUREL:\n",
            "I am son, I am subdle. I was one batters arorsed\n",
            "My learing. To please the heart makes a\n",
            "hances to the boy that onour to sufford me; you if\n",
            "actistreate for devolour I shall have his\n",
            "beat of your love's ill, and then will bear.\n",
            "\n",
            "DUKE OF SABERLET:\n",
            "This proclaim it, prithee, Time, make mecurety\n",
            "With you our runner pounted, and\n",
            "You answer fit o'er that the milesty,\n",
            "That trial he have been, with hangy one;\n",
            "And the night is trust a man is to buckner.\n",
            "\n",
            "OXFORD:\n",
            "There are the happy longer Duke of Norfolk\n",
            "For Kings Lancaster be my fiend.\n",
            "\n",
            "FLORD:\n",
            "Is't thou gonesable marry's pride Libmon, alone, and piras\n",
            "to me raime to her bike thus.\n",
            "\n",
            "VAUGILIA:\n",
            "'Frites being what calls thee inveter forgive.\n",
            "\n",
            "CLAUDIO:\n",
            "Then, no.\n",
            "Who hath thence?\n",
            "\n",
            "ISABELLA:\n",
            "This shall i' that then talk,\n",
            "Where I seon are with'sily show and doth other\n",
            "opposed?\n",
            "\n",
            "ASTINLEY:\n",
            "O Stoneford, dim did find, girtle their maje,\n",
            "As for Sail Coriolanus, Paris, making with you:\n",
            "So dispost to forswear along, we stand ip.\n",
            "\n",
            "Second Glord:\n",
            "You shall wish me to thy foot fortune and his man:\n",
            "Yiring nature,--and force will it idow\n",
            "This sought I mose my soodness business\n",
            "At lipttly to this bulk of blood:\n",
            "Thou leads, by the famous Camillo isle.a gone,\n",
            "Go dener\n",
            "A leaving inquirers herself, overwite, know\n",
            "sight have touch'd, though one to gentle; but there be not\n",
            "judge, puloung, an undone! but is deadded! what's quarrel?\n",
            "Thy, sudden with all te-king?\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "Heaven! and Clarence, fortune my tongue, most\n",
            "alive your honour, eve hurties from you\n",
            "deserved noke vows of ended usuring,\n",
            "Flattering bark with blood then mad outs.\n",
            "Go? let's these wrill languors, thy day's self,\n",
            "For is there for Jufitetus to was\n",
            "Show ours, from these tops weak gollising child;\n",
            "Confess his sort and her love many followed\n",
            "Roteo, and were true unvessed all part\n",
            "As fall of breath; and blacks other, whom\n",
            "With a like court: here sweets great coloudly matter,\n",
            "And, as not anythan to 'wisher, I that that?\n",
            "\n",
            "First Bagcond Salbury:\n",
            "With she, your hands to the foolering thence:\n",
            "The cunnot out af me, to death, as particulus\n",
            "Aslays like himly, for any poundted climic\n",
            "To our Bsalence; If any so shed other,\n",
            "And there's gold and my impassions, make thy power\n",
            "As venoon of their better had destranced\n",
            "As I in but a mourning; they ground here cafter\n",
            "To prepare him to the shore, if it may call the appared\n",
            "And disclate of breath\n",
            "A desolve you push: to brother grierous life\n",
            "Take wateward up his shiver, which we hate with me.\n",
            "I shall intend to fall, tave against bloody:\n",
            "Forn one shortony of her noise; cease thy heart?\n",
            "\n",
            "Third:\n",
            "There's not ack shallow? or my previl gof pent\n",
            "his liedness, serits who her shall nober seem that\n",
            "Since thee theresor is constant are us\n",
            "Confedrent doth; and to on me to ta. But, Wadward;\n",
            "Where meantal shall be not, came and us\n",
            "Shorts and sweet like a peace recompose,\n",
            "Which have tour loss a pecklar king:\n",
            "though he take enters, Romellburd words and not\n",
            "So let hir placed the wretch thou by to orce, for this\n",
            "but\n",
            "Reheaftering some troops.\n",
            "\n",
            "MENENIUS:\n",
            "This, sir: 'twas they congent promost noble,\n",
            "Nay, here made opentation men,\n",
            "Tell me as I am a means for them ever\n",
            "Such a wickled will safes of breats thieve ears.\n",
            "\n",
            "DUKE OF AUMERLE:\n",
            "Or have requests, you discture it among contempt\n",
            "To a twird seel fortune may cone than second\n",
            "Out himself mother's son? one thieves, I did,\n",
            "and none muster; lamention, and marry, Paugham here's\n",
            "Is not volse, and me, backs themselves, from me;\n",
            "But boys, and in them, you say look the ground;\n",
            "This ine the letter and appearved;\n",
            "I'll requel to-morrow was one badging,\n",
            "It my kneel'd. Thou hast knownly happy,\n",
            "And tolth our oath oracle; but the enjoir\n",
            "Upon joting cloud the vooter, and he hath\n",
            "Himself he speaks and give my free death conjund,\n",
            "Bascals. Come to-day; go prolove whee poor,\n",
            "If think call'd, and you adventagably,\n",
            "Were not Camillo his best.\n",
            "\n",
            "SLY:\n",
            "Why, his sorrow, sorrow in France, which fain bein so,\n",
            "to say it, as she o'er dream fresh my love:\n",
            "For weary then and clam whether I do\n",
            "Have well tops her more honour may on my house\n",
            "To pressess and Cotebmetnes the parts,\n",
            "Even his garliage, and do he comout a\n",
            "ne'er much tears.\n",
            "\n",
            "Pointer KINGBeens banishment your ctirns\n",
            "Ever also, I will atte\n"
          ]
        }
      ],
      "source": [
        "out = m.sample(10000)\n",
        "print(decode(out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSTM4F2uPFvk"
      },
      "outputs": [],
      "source": [
        "itos[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCDrQCwqAKgq"
      },
      "outputs": [],
      "source": [
        "a = torch.ones((3, 4, 5))\n",
        "b = torch.ones((3,5,4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8hpa1QIAfNR"
      },
      "outputs": [],
      "source": [
        "a.transpose(2, 1).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voTgVWlzAfpX"
      },
      "outputs": [],
      "source": [
        "b = 2*b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-Bh0aQ8A7Rl",
        "outputId": "9b1f71ad-6649-44b9-f1c6-47a97fa7bcf9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([3, 4, 4])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(a @ b).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsX_nt7bA9as",
        "outputId": "9b2c019a-3198-4aa4-f52f-bff95e95bf21"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1., 1., 1., 1., 1.])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a[0][0][-100:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gue0whkhFa4Y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XY0ifKF0EqFo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lb4VrTxiFHYn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4r_2UvbFkrg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GA59ByGWGB_R"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDuiwZxbGi9H"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlZXRwgnG_61"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMf/E7xW4aq+cTXX2dwHfAK",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}